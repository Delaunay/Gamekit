Machine Learning
----------------

Gamekit as a Reinforcement learning environment

* Smarter AI
* Competitive Scene boostraping
* Mixed Play
* AI <-> Human interaction trough chat wheel
* Automated Quality Assurance


Install prerequisite (Training)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note::

   Using a NVIDIA GPU (Pascal+, 10 series or newer) for this tutorial is recommended.

   As of 10/2021, the only supported consumer AMD-GPU supported are RadeonVII and Vega 64.
   RDNA1-2 are not yet supported.

   See `ROCm website <https://rocmdocs.amd.com/en/latest/>`_ for additional details.


- Install python using `conda`_
- Install `pytorch`_
- Install ue4ml (python)
- Enable UE4ML (C++/UE4 plugin)

.. code-block:: bash

   pip install -e . UnrealEngine/Engine/Plugins/AI/UE4ML/Source/python/


.. warning::

   ``ue4ml`` requires ``msgpack-rpc-python`` which is badly out of date.
   The required tornado version is particularly old ``tornado >= 3,<5``
   This might break your environment. You should build a virtualenv just for UE4.


.. note::

   To open anaconda with the right python environment you should use the shortcut that were created by anaconda

   .. code-block:: bash

      # Powershell
      %windir%\System32\WindowsPowerShell\v1.0\powershell.exe -ExecutionPolicy ByPass -NoExit -Command "& 'F:\anaconda\shell\condabin\conda-hook.ps1' ; conda activate 'F:\anaconda' "

      # Cmd
      %windir%\System32\cmd.exe "/K" F:\anaconda\Scripts\activate.bat F:\anaconda


Run ActionRPG example
~~~~~~~~~~~~~~~~~~~~~

* Donwload the `ActionRPG example <https://www.unrealengine.com/marketplace/en-US/product/action-rpg>`_ from the marketplace
* Move the project file to ``E:\UnrealEngine\ActionRPG``
* Generate the project files
* Compile project

.. code-block:: bash

   # Add UE-DevBinaries=E:/Engine/Binaries/Win64 to the path
   python E:\UnrealEngine\Engine\Plugins\AI\UE4ML\Source\python\examples\as_gym_env.py --env UE4-ActionRPG-v0

   # check path is correct
   dir E:/UnrealEngine/Engine/Binaries/Win64/UE4Editor.exe

   # if UE-DevBinaries is not in the path you can set it manually like bellow
   python E:\UnrealEngine\Engine\Plugins\AI\UE4ML\Source\python\examples\as_gym_env.py --env UE4-ActionRPG-v0 --exec E:/UnrealEngine/Engine/Binaries/Win64/UE4Editor.exe

   # if you encounter issues running the python script alone, you can try to launch the game inside the editor first
   # and connect to it using `connect_to_running.py`
   python E:\UnrealEngine\Engine\Plugins\AI\UE4ML\Source\python\examples\connect_to_running.py


.. note::

   You can find the logs of the game inside ``E:\UnrealEngine\ActionRPG\Saved\Logs\``.

   .. note::

      If the ActionRPG example crashes try to run manually a first time inside the editor.
      Some assets needs to be loaded before the project can be played.

      .. code-block::

         [2021.11.22-21.38.44:944][  0]LogWindows: Error: Assertion failed: !FUObjectThreadContext::Get().IsRoutingPostLoad [File:E:/UnrealEngine/Engine/Source/Runtime/CoreUObject/Private/UObject/ScriptCore.cpp] [Line: 1851]
         [2021.11.22-21.38.44:944][  0]LogWindows: Error: Cannot call UnrealScript (BP_GameInstance_C /Engine/Transient.GameEngine_0:BP_GameInstance_C_0 - Function /Game/Blueprints/BP_GameInstance.BP_GameInstance_C:Completed_FB1CB99B4EDC0A52723303B59941AADE) while PostLoading objects


.. note::

   when running the ActionRPG example, the initialization of UE4ML can fail.
   To make it run you can start the game inside the editor and execute the command ``4ml.server.restart 15151``.
   To attach to the running instance from python you will need to use a different python script ``connect_to_running.py``.


.. note::

   To debug the editor launch; simply copy paste the command printed by the python script and launch it using the command line

   .. code-block:: bash

      ENGINE_BINARIES=E:\UnrealEngine\Engine\Binaries\Win64
      PROJECT_BINARIES=E:\ActionRPG\Binaries\Win64          # <= WRONG
      PROJECT_BINARIES=E:\UnrealEngine\ActionRPG            # <= GOOD

      # Command line generated by ue4ml
      E:/UnrealEngine/Engine/Binaries/Win64/UE4Editor.exe ActionRPG ActionRPG_P -windowed -usefixedtimestep -game -unattended -nosound -resx=320 -resy=240 -fps=20 -4mlport=15151

      # To open a project from an arbitrary location
      E:/UnrealEngine/Engine/Binaries/Win64/UE4Editor.exe E:\Gamekit2\Chessy.uproject

      # To open a specific map
      # /Game/Levels/HideAndSeek/HideAndSeek.umap is located at E:\Gamekit2\Content\Levels\HideAndSeek\HideAndSeek.umap
      E:/UnrealEngine/Engine/Binaries/Win64/UE4Editor.exe E:\Gamekit2\Chessy.uproject /Game/Levels/HideAndSeek/HideAndSeek.umap

   .. note::

      see more command line options `here <https://docs.unrealengine.com/4.27/en-US/ProductionPipelines/CommandLineArguments/>`_


Making of a gym enviroment
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note::

   The example is available for download as a `standalone project <https://github.com/Delaunay/cartpole>`_


Basic Setup
^^^^^^^^^^^

* Add Input mapping ``CartMove``

   * A or Left scale -1
   * D or Right scale 1

* Create the entities below

* Level: CartPole
* PlayerController: CartPole_Controller (empty)
* PlayerState: Cart_State (needs to inherit from ``UE4RLPlayerState`` so the score can be updated using blueprints)
* Pawn Actor: Cart_Pawn

.. raw:: html

   <iframe width="100%" height="350px" src="https://blueprintue.com/render/i7gdsnhm/" scrolling="no" allowfullscreen></iframe>

* Pawn Actor: Pole_Pawn (needs to inherit from UE4RLPawn so it can be sighted by the AI perception system)

.. raw:: html

   <iframe width="100%" height="350px" src="https://blueprintue.com/render/4qwvkpbi/" scrolling="no" allowfullscreen></iframe>


* Mode: CartPole_Mode

.. raw:: html

   <iframe width="100%" height="350px" src="https://blueprintue.com/render/1g8l1wr9/" scrolling="no" allowfullscreen></iframe>


* In world setting set the mode to ``CartPole_Mode``

.. note::

   You should be able to play the "game" now


Define the action space
^^^^^^^^^^^^^^^^^^^^^^^

The action space is defined by adding ``U4MLActuator`` to the agent configuration.

* Available actuators:

   * InputKey: used the input mapping defined by the project
   * Camera: used to control the camera  (``AddPitchInput`` & ``AddYawInput``)


.. code-block:: python

   class CartPole(UnrealEnv):

       ...

       @staticmethod
       def default_agent_config():
          # Create a new agent config
          agent_config = AgentConfig()

          # Set the spawn class that is being controlled
          agent_config.avatarClassName = "CartPole_Pawn_C"

          # Define the action space by adding actuators
          agent_config.add_actuator("InputKey")


For our CartPole example this will result in a an action space of `Discrete(2)`
since only 1 Axis input that varies between -1 and 1.


Define the observation space
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The observation space is defined by adding ``U4MLSensor`` to the agent configuration.


* Available sensors:

   * AIPerception: hook itself to the AI Perception system of UE4 (Note this is a Game-AI (Behavior Trees) not ML-AI)

      * see `AI Perception <https://docs.unrealengine.com/4.27/en-US/InteractiveExperiences/ArtificialIntelligence/AIPerception/>`_ which include Hearing, Sight, Team (proximity of ally), Touch. Currently only Sight is supported, it is created by the UE4ML system and added to the player controller.

   * Attribute: listen to attribute change if you are using `UAttributeSet <https://docs.unrealengine.com/4.27/en-US/API/Plugins/GameplayAbilities/UAttributeSet/>`_ for your character.
   * Camera: Make a camera capture of the scene
   * Input: Capture the inputs
   * Movement: Capture the characters movement & acceleration

      * Space: ``Box([-1. -1. -1. -1. -1. -1.], [1. 1. 1. 1. 1. 1.], (6,), float32)``
      * 3 floats for the positions and another 3 floats for the acceleration


.. code-block:: python

   class CartPole(UnrealEnv):

       ...

       @staticmethod
       def default_agent_config():
          # Create a new agent config
          agent_config = AgentConfig()

          # Set the spawn class that is being controlled
          agent_config.avatarClassName = "Cart_Pawn_C"

          # Actuators
          ...

          # Define the observation space by adding sensors

          # Add our pawn movement (i.e cart movement)
          agent_config.add_sensor(
              "Movement",
              {
                  "location": "absolute",
                  "velocity": "absolute"
              }
          )

          # Add sight so we can see the pole

          agent_config.add_sensor(
                "AIPerception",
                {
                   "count": "1",                   # Number of actors it can see
                   'sort': 'distance',             # how the actors are sorted `distance`` or `in_front`
                   'peripheral_angle': 360,        # sight cone
                   'mode': 'vector',               # vector (HeadingVector) or rotator
                                                   # max_age
                }
          )

.. code-block:: python

   # Observation space
   Tuple(
      # AIPerception
      Box([-1. -1. -1. -1. -1.], [1. 1. 1. 1. 1.], (5,), float32),

      # Movement
      Box([-1. -1. -1. -1. -1. -1.], [1. 1. 1. 1. 1. 1.], (6,), float32)
   )

   # Observation
   (
      array([ 9.8459434e-41,  3.9260104e+02,  9.6790361e-01, -2.3592940e-01, -8.6601958e-02], dtype=float32),
      array([    240.      ,      90.84363 ,      242.00069 ,      0.      ,    -77.921715,     0.      ], dtype=float32)
   )

.. warning::

   The sight sensor has an affiliation property that can filter out between friendlies/hostiles and neutrals.
   If the ``AIPerception`` observation is not set that would be the main reason why.

   The affiliation is set using ``ETeamAttitude`` from the ``FGenericTeamId``
   The team id is returned using ``FGenericTeamId FGenericTeamId::GetTeamIdentifier(const AActor* TeamMember)``
   The ``AActor`` must implement the ``IGenericTeamAgentInterface`` interface (if not ``FGenericTeamId::NoTeam`` is used).


Define a custom reward
^^^^^^^^^^^^^^^^^^^^^^

A simple approach that does not require to modify the game code is to configure
the observation space so the reward can be computed for each given observsation,
as such the custom reward can be computed in the python script itself.
This approach can be cumbersome to implement as the observation space is not annotated.

To provide a custom reward in UnrealEngine, you need to set a ``APlayerState`` inside the controller (Created by default).
``APlayerState::GetScore`` will be used as the reward. ``APlayerState::SetScore`` is not exposed in blueprints so you will have
to create a custom C++ ``APlayerState``.

In our example we use the X rotation angle (Roll) as the reward.


Training
^^^^^^^^




Wishlist
~~~~~~~~

* open-sourced ue4ml python package
* be able to pick a '.uproject' (you can make it happen already by using ``PROJECT_NAME`` as a path) instead of relying on an implicit path ('UnrealEngine/<PROJECT_NAME>')
* make it easier to select a map per project (one project could have multiple environment, specially when they are fairly small)
* Parallel Environment support
* Support all AI Perception senses
* Make AI Sight config ``DetectionByAffiliation`` available (avoid making user specialize a Pawn just for RL)
* Annotate the observation & action space

   * The observation space order is unclear. Python dictionnary are ordered the observation space returned is ordered differently
   the order look deterministic but it would be nice it it was made explicit.
   sensors are stored in a ``TMap`` so the order in which the space is generated is not clear.
   Additionally they are probably json-searialized so the order could be switched again there.


.. note::

   Because the space is generated in the C++ (with the funky ordering) and then sent to python
   maybe the ordering is set by the space generation and as such no bug could occur.
   But I still think having control over the ordering will be powerful.
   Specially the space generation must be deterministic so the network that is
   trained is fully reusable.


References
~~~~~~~~~~

* `UE4ML`_
* `Readme`_
* `AirSim <https://github.com/microsoft/AirSim>`_

.. _MI: https://www.amd.com/en/graphics/instinct-server-accelerators
.. _pytorch: https://pytorch.org/get-started/locally/
.. _conda: https://docs.conda.io/en/latest/miniconda.html
.. _UE4ML: https://docs.unrealengine.com/4.27/en-US/API/Plugins/UE4ML/
.. _Readme: https://github.com/EpicGames/UnrealEngine/tree/release/Engine/Plugins/AI/UE4ML
